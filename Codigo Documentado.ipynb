{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'imagep' from 'keras.preprocessing' (/Users/felipereus/anaconda3/envs/eda_env/lib/python3.9/site-packages/keras/preprocessing/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/felipereus/Desktop/DATA SCIENCE/Proyecto Mlearning/Codigo Documentado.ipynb Cell 2\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/felipereus/Desktop/DATA%20SCIENCE/Proyecto%20Mlearning/Codigo%20Documentado.ipynb#W0sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m Dense, Flatten\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/felipereus/Desktop/DATA%20SCIENCE/Proyecto%20Mlearning/Codigo%20Documentado.ipynb#W0sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapplications\u001b[39;00m \u001b[39mimport\u001b[39;00m VGG16\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/felipereus/Desktop/DATA%20SCIENCE/Proyecto%20Mlearning/Codigo%20Documentado.ipynb#W0sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m imagep \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/felipereus/Desktop/DATA%20SCIENCE/Proyecto%20Mlearning/Codigo%20Documentado.ipynb#W0sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m BatchNormalization\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/felipereus/Desktop/DATA%20SCIENCE/Proyecto%20Mlearning/Codigo%20Documentado.ipynb#W0sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m Conv2D\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'imagep' from 'keras.preprocessing' (/Users/felipereus/anaconda3/envs/eda_env/lib/python3.9/site-packages/keras/preprocessing/__init__.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from keras.preprocessing import imagep \n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Conv2DTranspose\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filename' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/felipereus/Desktop/DATA SCIENCE/Proyecto Mlearning/Codigo Documentado.ipynb Cell 3\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/felipereus/Desktop/DATA%20SCIENCE/Proyecto%20Mlearning/Codigo%20Documentado.ipynb#W1sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# Inicia un bucle que recorre cada archivo en la lista 'filenames'.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/felipereus/Desktop/DATA%20SCIENCE/Proyecto%20Mlearning/Codigo%20Documentado.ipynb#W1sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mfor\u001b[39;00m Training \u001b[39min\u001b[39;00m Training:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/felipereus/Desktop/DATA%20SCIENCE/Proyecto%20Mlearning/Codigo%20Documentado.ipynb#W1sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39m# Extrae la categoría del nombre del archivo, asumiendo que el nombre del archivo comienza con la categoría seguida de un punto.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/felipereus/Desktop/DATA%20SCIENCE/Proyecto%20Mlearning/Codigo%20Documentado.ipynb#W1sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39m# Ejemplo: en 'dog.123.jpg', 'dog' sería la categoría.\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/felipereus/Desktop/DATA%20SCIENCE/Proyecto%20Mlearning/Codigo%20Documentado.ipynb#W1sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     category \u001b[39m=\u001b[39m filename\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/felipereus/Desktop/DATA%20SCIENCE/Proyecto%20Mlearning/Codigo%20Documentado.ipynb#W1sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     \u001b[39m# Añade la categoría extraída a la lista 'categories'.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/felipereus/Desktop/DATA%20SCIENCE/Proyecto%20Mlearning/Codigo%20Documentado.ipynb#W1sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     categories\u001b[39m.\u001b[39mappend(category)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'filename' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    " y prueba.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "TRAIN_PATH = '../Proyecto Mlearning/DataSet/Training'\n",
    "\n",
    "Training = os.listdir(TRAIN_PATH)\n",
    "\n",
    "categories = []\n",
    "\n",
    "for Training in Training:\n",
    "    \n",
    "    category = filename.split('.')[0]\n",
    "\n",
    "    categories.append(category)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'filenames': filenames,\n",
    "    'category': categories\n",
    "})\n",
    "\n",
    "train_df, validate_df = train_test_split(df,\n",
    "                                         test_size=0.20,\n",
    "                                         random_state=42)\n",
    "\n",
    "\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "validate_df = validate_df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ------------------------------ Modelo Creado y ajustando parametros ------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4068 files belonging to 4 classes.\n",
      "Found 4068 files belonging to 4 classes.\n",
      "Using 3255 files for training.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "If using `validation_split` and shuffling the data, you must provide a `seed` argument, to make sure that there is no overlap between the training and validation subset.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/felipereus/Desktop/DATA SCIENCE/Proyecto Mlearning/Codigo Documentado.ipynb Cell 5\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/felipereus/Desktop/DATA%20SCIENCE/Proyecto%20Mlearning/Codigo%20Documentado.ipynb#W2sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# Creación del conjunto de datos con preprocesamiento\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/felipereus/Desktop/DATA%20SCIENCE/Proyecto%20Mlearning/Codigo%20Documentado.ipynb#W2sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m train_dataset \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mpreprocessing\u001b[39m.\u001b[39mimage_dataset_from_directory(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/felipereus/Desktop/DATA%20SCIENCE/Proyecto%20Mlearning/Codigo%20Documentado.ipynb#W2sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     TRAIN_PATH,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/felipereus/Desktop/DATA%20SCIENCE/Proyecto%20Mlearning/Codigo%20Documentado.ipynb#W2sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     validation_split\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/felipereus/Desktop/DATA%20SCIENCE/Proyecto%20Mlearning/Codigo%20Documentado.ipynb#W2sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     label_mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcategorical\u001b[39m\u001b[39m'\u001b[39m \u001b[39m# Para clasificación multiclase\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/felipereus/Desktop/DATA%20SCIENCE/Proyecto%20Mlearning/Codigo%20Documentado.ipynb#W2sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m )\u001b[39m.\u001b[39mmap(\u001b[39mlambda\u001b[39;00m x, y: (preprocessing_layer(x), y))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/felipereus/Desktop/DATA%20SCIENCE/Proyecto%20Mlearning/Codigo%20Documentado.ipynb#W2sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m validation_dataset \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mpreprocessing\u001b[39m.\u001b[39;49mimage_dataset_from_directory(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/felipereus/Desktop/DATA%20SCIENCE/Proyecto%20Mlearning/Codigo%20Documentado.ipynb#W2sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     TRAIN_PATH,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/felipereus/Desktop/DATA%20SCIENCE/Proyecto%20Mlearning/Codigo%20Documentado.ipynb#W2sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     validation_split\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/felipereus/Desktop/DATA%20SCIENCE/Proyecto%20Mlearning/Codigo%20Documentado.ipynb#W2sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     subset\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mvalidation\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/felipereus/Desktop/DATA%20SCIENCE/Proyecto%20Mlearning/Codigo%20Documentado.ipynb#W2sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     \u001b[39m#seed=123,\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/felipereus/Desktop/DATA%20SCIENCE/Proyecto%20Mlearning/Codigo%20Documentado.ipynb#W2sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     image_size\u001b[39m=\u001b[39;49m(\u001b[39m224\u001b[39;49m, \u001b[39m224\u001b[39;49m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/felipereus/Desktop/DATA%20SCIENCE/Proyecto%20Mlearning/Codigo%20Documentado.ipynb#W2sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     batch_size\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/felipereus/Desktop/DATA%20SCIENCE/Proyecto%20Mlearning/Codigo%20Documentado.ipynb#W2sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     label_mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcategorical\u001b[39;49m\u001b[39m'\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/felipereus/Desktop/DATA%20SCIENCE/Proyecto%20Mlearning/Codigo%20Documentado.ipynb#W2sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m )\u001b[39m.\u001b[39mmap(\u001b[39mlambda\u001b[39;00m x, y: (preprocessing_layer(x), y))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/felipereus/Desktop/DATA%20SCIENCE/Proyecto%20Mlearning/Codigo%20Documentado.ipynb#W2sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39m# Construcción del modelo\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/felipereus/Desktop/DATA%20SCIENCE/Proyecto%20Mlearning/Codigo%20Documentado.ipynb#W2sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m model \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mmodels\u001b[39m.\u001b[39mSequential([\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/felipereus/Desktop/DATA%20SCIENCE/Proyecto%20Mlearning/Codigo%20Documentado.ipynb#W2sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     \u001b[39m# Agregar capas aquí\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/felipereus/Desktop/DATA%20SCIENCE/Proyecto%20Mlearning/Codigo%20Documentado.ipynb#W2sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mConv2D(\u001b[39m32\u001b[39m, (\u001b[39m3\u001b[39m, \u001b[39m3\u001b[39m), activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m, input_shape\u001b[39m=\u001b[39m(\u001b[39m224\u001b[39m, \u001b[39m224\u001b[39m, \u001b[39m3\u001b[39m)),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/felipereus/Desktop/DATA%20SCIENCE/Proyecto%20Mlearning/Codigo%20Documentado.ipynb#W2sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m     tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mDense(\u001b[39m4\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msoftmax\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m# Ajustar según el número de clases\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/felipereus/Desktop/DATA%20SCIENCE/Proyecto%20Mlearning/Codigo%20Documentado.ipynb#W2sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m ])\n",
      "File \u001b[0;32m~/anaconda3/envs/eda_env/lib/python3.9/site-packages/keras/src/utils/image_dataset.py:207\u001b[0m, in \u001b[0;36mimage_dataset_from_directory\u001b[0;34m(directory, labels, label_mode, class_names, color_mode, batch_size, image_size, shuffle, seed, validation_split, subset, interpolation, follow_links, crop_to_aspect_ratio, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    203\u001b[0m         \u001b[39m'\u001b[39m\u001b[39m`color_mode` must be one of \u001b[39m\u001b[39m{\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mrgb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mrgba\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgrayscale\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m}. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    204\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mReceived: color_mode=\u001b[39m\u001b[39m{\u001b[39;00mcolor_mode\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    205\u001b[0m     )\n\u001b[1;32m    206\u001b[0m interpolation \u001b[39m=\u001b[39m image_utils\u001b[39m.\u001b[39mget_interpolation(interpolation)\n\u001b[0;32m--> 207\u001b[0m dataset_utils\u001b[39m.\u001b[39;49mcheck_validation_split_arg(\n\u001b[1;32m    208\u001b[0m     validation_split, subset, shuffle, seed\n\u001b[1;32m    209\u001b[0m )\n\u001b[1;32m    211\u001b[0m \u001b[39mif\u001b[39;00m seed \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    212\u001b[0m     seed \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandint(\u001b[39m1e6\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/eda_env/lib/python3.9/site-packages/keras/src/utils/dataset_utils.py:746\u001b[0m, in \u001b[0;36mcheck_validation_split_arg\u001b[0;34m(validation_split, subset, shuffle, seed)\u001b[0m\n\u001b[1;32m    741\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    742\u001b[0m         \u001b[39m'\u001b[39m\u001b[39m`subset` must be either \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtraining\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    743\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mvalidation\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m or \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mboth\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, received: \u001b[39m\u001b[39m{\u001b[39;00msubset\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m    744\u001b[0m     )\n\u001b[1;32m    745\u001b[0m \u001b[39mif\u001b[39;00m validation_split \u001b[39mand\u001b[39;00m shuffle \u001b[39mand\u001b[39;00m seed \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 746\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    747\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIf using `validation_split` and shuffling the data, you must \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    748\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mprovide a `seed` argument, to make sure that there is no \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    749\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39moverlap between the training and validation subset.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    750\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: If using `validation_split` and shuffling the data, you must provide a `seed` argument, to make sure that there is no overlap between the training and validation subset."
     ]
    }
   ],
   "source": [
    "# Importa la biblioteca pandas con el alias 'pd'. Pandas es utilizada para la manipulación y análisis de datos.\n",
    "import pandas as pd\n",
    "\n",
    "# Importa la función train_test_split desde sklearn.model_selection. Esta función se utiliza para dividir datos en conjuntos de entrenamiento y prueba.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define una constante IM_SIZE con el valor 32. Esta constante podría usarse para establecer un tamaño estándar para las imágenes, aunque no se usa en este fragmento de código.\n",
    "#IM_SIZE=32\n",
    "\n",
    "# Define una variable TRAIN_PATH con la ruta del directorio que contiene los datos de entrenamiento\n",
    "TRAIN_PATH = '../Proyecto Mlearning/DataSet/Training'\n",
    "\n",
    "# asigno tamaño y mejora de la estabilidad del modelo\n",
    "train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    TRAIN_PATH,\n",
    "    image_size=(224, 224),\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "\n",
    "# Preprocesamiento y Data Augmentation (opcional)\n",
    "preprocessing_layer = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)\n",
    "\n",
    "# Creación del conjunto de datos con preprocesamiento\n",
    "train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    TRAIN_PATH,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=(224, 224),\n",
    "    batch_size=64,\n",
    "    label_mode='categorical' # Para clasificación multiclase\n",
    ").map(lambda x, y: (preprocessing_layer(x), y))\n",
    "\n",
    "validation_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    TRAIN_PATH,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    #seed=123,\n",
    "    image_size=(224, 224),\n",
    "    batch_size=64,\n",
    "    label_mode='categorical'\n",
    ").map(lambda x, y: (preprocessing_layer(x), y))\n",
    "\n",
    "# Construcción del modelo\n",
    "model = tf.keras.models.Sequential([\n",
    "    # Agregar capas aquí\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    # ... más capas ...\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(4, activation='softmax') # Ajustar según el número de clases\n",
    "])\n",
    "\n",
    "# Compilación del modelo\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=validation_dataset,\n",
    "    epochs=10\n",
    ")\n",
    "\n",
    "# Puedes agregar código para guardar el modelo, evaluarlo, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#      ----------------------     Porbando otro Modele    ---------------------- \n",
    "# imagenes a 224 x 224 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4068 files belonging to 4 classes.\n",
      "Found 4068 files belonging to 4 classes.\n",
      "Using 3255 files for training.\n",
      "Found 4068 files belonging to 4 classes.\n",
      "Using 813 files for validation.\n"
     ]
    }
   ],
   "source": [
    "# Importa la biblioteca pandas con el alias 'pd'. Pandas es utilizada para la manipulación y análisis de datos.\n",
    "import pandas as pd\n",
    "\n",
    "# Importa la función train_test_split desde sklearn.model_selection. Esta función se utiliza para dividir datos en conjuntos de entrenamiento y prueba.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define una constante IM_SIZE con el valor 32. Esta constante podría usarse para establecer un tamaño estándar para las imágenes, aunque no se usa en este fragmento de código.\n",
    "#IM_SIZE=32\n",
    "\n",
    "# Define una variable TRAIN_PATH con la ruta del directorio que contiene los datos de entrenamiento\n",
    "TRAIN_PATH = '../Proyecto Mlearning/DataSet/Training'\n",
    "\n",
    "# asigno tamaño y mejora de la estabilidad del modelo\n",
    "train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    TRAIN_PATH,\n",
    "    image_size=(100, 100),\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "\n",
    "# Preprocesamiento y Data Augmentation (opcional)\n",
    "preprocessing_layer = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)\n",
    "\n",
    "# Creación del conjunto de datos con preprocesamiento\n",
    "train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    TRAIN_PATH,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=(100, 100),\n",
    "    batch_size=64,\n",
    "    label_mode='categorical' # Para clasificación multiclase\n",
    ").map(lambda x, y: (preprocessing_layer(x), y))\n",
    "\n",
    "validation_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    TRAIN_PATH,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    image_size=(100, 100),\n",
    "    batch_size=64,\n",
    "    label_mode='categorical'\n",
    ").map(lambda x, y: (preprocessing_layer(x), y))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(100, 100, 3))\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    Flatten(),\n",
    "    Dense(1024, activation='relu'),\n",
    "    Dense(4, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.engine.functional.Functional at 0x2966772b0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "51/51 [==============================] - 161s 3s/step - loss: 0.5668 - accuracy: 0.8467 - val_loss: 0.0255 - val_accuracy: 0.9975\n",
      "Epoch 2/5\n",
      "51/51 [==============================] - ETA: 0s - loss: 0.0121 - accuracy: 1.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/felipereus/Desktop/DATA SCIENCE/Proyecto Mlearning/Codigo Documentado.ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/felipereus/Desktop/DATA%20SCIENCE/Proyecto%20Mlearning/Codigo%20Documentado.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcategorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/felipereus/Desktop/DATA%20SCIENCE/Proyecto%20Mlearning/Codigo%20Documentado.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/felipereus/Desktop/DATA%20SCIENCE/Proyecto%20Mlearning/Codigo%20Documentado.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     train_dataset, \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/felipereus/Desktop/DATA%20SCIENCE/Proyecto%20Mlearning/Codigo%20Documentado.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/felipereus/Desktop/DATA%20SCIENCE/Proyecto%20Mlearning/Codigo%20Documentado.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     validation_data\u001b[39m=\u001b[39;49mvalidation_dataset\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/felipereus/Desktop/DATA%20SCIENCE/Proyecto%20Mlearning/Codigo%20Documentado.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/eda_env/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/eda_env/lib/python3.9/site-packages/keras/src/engine/training.py:1856\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1840\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m_eval_data_handler\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1841\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eval_data_handler \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39mget_data_handler(\n\u001b[1;32m   1842\u001b[0m         x\u001b[39m=\u001b[39mval_x,\n\u001b[1;32m   1843\u001b[0m         y\u001b[39m=\u001b[39mval_y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1854\u001b[0m         pss_evaluation_shards\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pss_evaluation_shards,\n\u001b[1;32m   1855\u001b[0m     )\n\u001b[0;32m-> 1856\u001b[0m val_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluate(\n\u001b[1;32m   1857\u001b[0m     x\u001b[39m=\u001b[39;49mval_x,\n\u001b[1;32m   1858\u001b[0m     y\u001b[39m=\u001b[39;49mval_y,\n\u001b[1;32m   1859\u001b[0m     sample_weight\u001b[39m=\u001b[39;49mval_sample_weight,\n\u001b[1;32m   1860\u001b[0m     batch_size\u001b[39m=\u001b[39;49mvalidation_batch_size \u001b[39mor\u001b[39;49;00m batch_size,\n\u001b[1;32m   1861\u001b[0m     steps\u001b[39m=\u001b[39;49mvalidation_steps,\n\u001b[1;32m   1862\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m   1863\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[1;32m   1864\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[1;32m   1865\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[1;32m   1866\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1867\u001b[0m     _use_cached_eval_dataset\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1868\u001b[0m )\n\u001b[1;32m   1869\u001b[0m val_logs \u001b[39m=\u001b[39m {\n\u001b[1;32m   1870\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mval_\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m name: val \u001b[39mfor\u001b[39;00m name, val \u001b[39min\u001b[39;00m val_logs\u001b[39m.\u001b[39mitems()\n\u001b[1;32m   1871\u001b[0m }\n\u001b[1;32m   1872\u001b[0m epoch_logs\u001b[39m.\u001b[39mupdate(val_logs)\n",
      "File \u001b[0;32m~/anaconda3/envs/eda_env/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/eda_env/lib/python3.9/site-packages/keras/src/engine/training.py:2296\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   2292\u001b[0m             \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   2293\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m, step_num\u001b[39m=\u001b[39mstep, _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m   2294\u001b[0m             ):\n\u001b[1;32m   2295\u001b[0m                 callbacks\u001b[39m.\u001b[39mon_test_batch_begin(step)\n\u001b[0;32m-> 2296\u001b[0m                 logs \u001b[39m=\u001b[39m test_function_runner\u001b[39m.\u001b[39;49mrun_step(\n\u001b[1;32m   2297\u001b[0m                     dataset_or_iterator,\n\u001b[1;32m   2298\u001b[0m                     data_handler,\n\u001b[1;32m   2299\u001b[0m                     step,\n\u001b[1;32m   2300\u001b[0m                     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_pss_evaluation_shards,\n\u001b[1;32m   2301\u001b[0m                 )\n\u001b[1;32m   2303\u001b[0m logs \u001b[39m=\u001b[39m tf_utils\u001b[39m.\u001b[39msync_to_numpy_or_python_type(logs)\n\u001b[1;32m   2304\u001b[0m \u001b[39m# Override with model metrics instead of last step logs\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/eda_env/lib/python3.9/site-packages/keras/src/engine/training.py:4108\u001b[0m, in \u001b[0;36m_TestFunction.run_step\u001b[0;34m(self, dataset_or_iterator, data_handler, step, unused_shards)\u001b[0m\n\u001b[1;32m   4107\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_step\u001b[39m(\u001b[39mself\u001b[39m, dataset_or_iterator, data_handler, step, unused_shards):\n\u001b[0;32m-> 4108\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_function(dataset_or_iterator)\n\u001b[1;32m   4109\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   4110\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/anaconda3/envs/eda_env/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/eda_env/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    829\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    831\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 832\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    834\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    835\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/anaconda3/envs/eda_env/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:877\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    875\u001b[0m \u001b[39m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    876\u001b[0m \u001b[39m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 877\u001b[0m results \u001b[39m=\u001b[39m tracing_compilation\u001b[39m.\u001b[39;49mcall_function(\n\u001b[1;32m    878\u001b[0m     args, kwds, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_variable_creation_config\n\u001b[1;32m    879\u001b[0m )\n\u001b[1;32m    880\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_created_variables:\n\u001b[1;32m    881\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCreating variables on a non-first call to a function\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    882\u001b[0m                    \u001b[39m\"\u001b[39m\u001b[39m decorated with tf.function.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/eda_env/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[39m=\u001b[39m function\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39mbind(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[39m=\u001b[39m function\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[39mreturn\u001b[39;00m function\u001b[39m.\u001b[39;49m_call_flat(  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[39m=\u001b[39;49mfunction\u001b[39m.\u001b[39;49mcaptured_inputs\n\u001b[1;32m    141\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/eda_env/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1319\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1320\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1321\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1322\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1323\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall_preflattened(args)\n\u001b[1;32m   1324\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m     args,\n\u001b[1;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1327\u001b[0m     executing_eagerly)\n\u001b[1;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/anaconda3/envs/eda_env/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall_preflattened\u001b[39m(\u001b[39mself\u001b[39m, args: Sequence[core\u001b[39m.\u001b[39mTensor]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcall_flat(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    217\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/eda_env/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[1;32m    252\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[1;32m    253\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[1;32m    254\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[1;32m    255\u001b[0m     )\n\u001b[1;32m    256\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[39mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mfunction_call_options\u001b[39m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/eda_env/lib/python3.9/site-packages/tensorflow/python/eager/context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[1;32m   1485\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1486\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m   1487\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1488\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[1;32m   1489\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[1;32m   1490\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m   1491\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m   1492\u001b[0m   )\n\u001b[1;32m   1493\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1494\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1495\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m   1496\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1500\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[1;32m   1501\u001b[0m   )\n",
      "File \u001b[0;32m~/anaconda3/envs/eda_env/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(\n",
    "    train_dataset, \n",
    "    epochs=5,\n",
    "    validation_data=validation_dataset\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ------------------------------ Modelo Creado y ajustando parametros ------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4068 images belonging to 4 classes.\n",
      "Found 0 images belonging to 4 classes.\n",
      "Epoch 1/10\n",
      "63/63 [==============================] - 65s 1s/step - loss: 5.8842 - accuracy: 0.5177\n",
      "Epoch 2/10\n",
      "63/63 [==============================] - 67s 1s/step - loss: 1.0267 - accuracy: 0.7832\n",
      "Epoch 3/10\n",
      "63/63 [==============================] - 63s 1s/step - loss: 0.7690 - accuracy: 0.8449\n",
      "Epoch 4/10\n",
      "63/63 [==============================] - 62s 973ms/step - loss: 0.6479 - accuracy: 0.8621\n",
      "Epoch 5/10\n",
      "63/63 [==============================] - 65s 1s/step - loss: 0.5449 - accuracy: 0.8926\n",
      "Epoch 6/10\n",
      "63/63 [==============================] - 64s 1s/step - loss: 0.4785 - accuracy: 0.8944\n",
      "Epoch 7/10\n",
      "63/63 [==============================] - 60s 948ms/step - loss: 0.4081 - accuracy: 0.9076\n",
      "Epoch 8/10\n",
      "63/63 [==============================] - 70s 1s/step - loss: 0.3905 - accuracy: 0.9123\n",
      "Epoch 9/10\n",
      "63/63 [==============================] - 64s 1s/step - loss: 0.3541 - accuracy: 0.9133\n",
      "Epoch 10/10\n",
      "63/63 [==============================] - 63s 996ms/step - loss: 0.3518 - accuracy: 0.9153\n"
     ]
    }
   ],
   "source": [
    "# Importaciones necesarias\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define una variable TRAIN_PATH con la ruta del directorio que contiene los datos de entrenamiento\n",
    "TRAIN_PATH = '../Proyecto Mlearning/DataSet/Training'\n",
    "\n",
    "# Parámetros de Data Augmentation para el generador de datos de entrenamiento\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255.,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "# Generador de datos de validación (sin data augmentation)\n",
    "validation_datagen = ImageDataGenerator(rescale=1.0/255.)\n",
    "\n",
    "# Creación del conjunto de datos de entrenamiento y validación con Data Augmentation\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    TRAIN_PATH,\n",
    "    subset=\"training\",\n",
    "    #seed=123,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=64,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    TRAIN_PATH,\n",
    "    subset=\"validation\",\n",
    "    #seed=123,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=64,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Construcción del modelo con Dropout y Regularización L2\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    # Puedes agregar más capas convolucionales y de pooling si es necesario\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu', kernel_regularizer=l2(0.001)),  # Regularización L2\n",
    "    Dropout(0.5),  # Capa de Dropout\n",
    "    Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compilación del modelo\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=10,\n",
    "    steps_per_epoch=train_generator.n // train_generator.batch_size,\n",
    "    validation_steps=validation_generator.n // validation_generator.batch_size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3256 images belonging to 4 classes.\n",
      "Found 812 images belonging to 4 classes.\n",
      "Epoch 1/10\n",
      "50/50 [==============================] - 56s 1s/step - loss: 5.8391 - accuracy: 0.5442 - val_loss: 1.1533 - val_accuracy: 0.6992\n",
      "Epoch 2/10\n",
      "50/50 [==============================] - 51s 1s/step - loss: 0.9949 - accuracy: 0.7945 - val_loss: 0.7345 - val_accuracy: 0.9844\n",
      "Epoch 3/10\n",
      "50/50 [==============================] - 57s 1s/step - loss: 0.7844 - accuracy: 0.8421 - val_loss: 0.6080 - val_accuracy: 0.8711\n",
      "Epoch 4/10\n",
      "50/50 [==============================] - 57s 1s/step - loss: 0.6989 - accuracy: 0.8434 - val_loss: 0.5897 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "50/50 [==============================] - 58s 1s/step - loss: 0.6222 - accuracy: 0.8669 - val_loss: 0.4216 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "50/50 [==============================] - 61s 1s/step - loss: 0.5301 - accuracy: 0.8935 - val_loss: 0.3780 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "50/50 [==============================] - 60s 1s/step - loss: 0.5365 - accuracy: 0.8659 - val_loss: 0.3315 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "50/50 [==============================] - 59s 1s/step - loss: 0.4467 - accuracy: 0.9032 - val_loss: 0.2735 - val_accuracy: 0.9792\n",
      "Epoch 9/10\n",
      "50/50 [==============================] - 61s 1s/step - loss: 0.4714 - accuracy: 0.8769 - val_loss: 0.2515 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "50/50 [==============================] - 64s 1s/step - loss: 0.4337 - accuracy: 0.8681 - val_loss: 0.2186 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Importaciones necesarias\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define una variable TRAIN_PATH con la ruta del directorio que contiene los datos de entrenamiento\n",
    "TRAIN_PATH = '../Proyecto Mlearning/DataSet/Training'\n",
    "\n",
    "# Crear un ImageDataGenerator con los parámetros de data augmentation para el entrenamiento\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255.,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2  # Aquí especificas el porcentaje para validación\n",
    ")\n",
    "\n",
    "# Crear un ImageDataGenerator para la validación (sin data augmentation)\n",
    "validation_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255.,\n",
    "    validation_split=0.2  # Especificar el mismo porcentaje que para el entrenamiento\n",
    ")\n",
    "\n",
    "# Crear el generador de datos de entrenamiento\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    TRAIN_PATH,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=64,\n",
    "    class_mode='categorical',\n",
    "    subset='training',  # Especificar el subset de entrenamiento\n",
    "    seed=123\n",
    ")\n",
    "\n",
    "# Crear el generador de datos de validación\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    TRAIN_PATH,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=64,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',  # Especificar el subset de validación\n",
    "    seed=123\n",
    ")\n",
    "\n",
    "# Construcción del modelo (igual\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    # Puedes agregar más capas convolucionales y de pooling si es necesario\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu', kernel_regularizer=l2(0.001)),  # Regularización L2\n",
    "    Dropout(0.5),  # Capa de Dropout\n",
    "    Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compilación del modelo\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"callback_model.h5\")\n",
    "history = model.fit(X_train,\n",
    "                   y_train,\n",
    "                   epochs=30,\n",
    "                   callbacks = [checkpoint_cb])\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=10,\n",
    "    steps_per_epoch=train_generator.n // train_generator.batch_size,\n",
    "    validation_steps=validation_generator.n // validation_generator.batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_21 (Conv2D)          (None, 222, 222, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d_21 (MaxPooli  (None, 111, 111, 32)      0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " flatten_25 (Flatten)        (None, 394272)            0         \n",
      "                                                                 \n",
      " dense_50 (Dense)            (None, 128)               50466944  \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_51 (Dense)            (None, 4)                 516       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 50468356 (192.52 MB)\n",
      "Trainable params: 50468356 (192.52 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   ---------------------- modelo ajustado para subir val_Accuracy ------------------------    CODIGO A USAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3256 images belonging to 4 classes.\n",
      "Found 812 images belonging to 4 classes.\n",
      "Epoch 1/15\n",
      "50/50 [==============================] - 21s 409ms/step - loss: 0.6671 - accuracy: 0.7215 - val_loss: 1.0761 - val_accuracy: 0.4271\n",
      "Epoch 2/15\n",
      "50/50 [==============================] - 22s 448ms/step - loss: 0.0823 - accuracy: 0.9740 - val_loss: 0.0027 - val_accuracy: 1.0000\n",
      "Epoch 3/15\n",
      "50/50 [==============================] - 20s 406ms/step - loss: 0.0050 - accuracy: 0.9987 - val_loss: 3.5278e-04 - val_accuracy: 1.0000\n",
      "Epoch 4/15\n",
      "50/50 [==============================] - 20s 402ms/step - loss: 0.0027 - accuracy: 0.9997 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 5/15\n",
      "50/50 [==============================] - 21s 413ms/step - loss: 9.2874e-04 - accuracy: 1.0000 - val_loss: 9.7881e-04 - val_accuracy: 1.0000\n",
      "Epoch 6/15\n",
      "50/50 [==============================] - 20s 391ms/step - loss: 3.9699e-04 - accuracy: 1.0000 - val_loss: 4.8535e-04 - val_accuracy: 1.0000\n",
      "Epoch 7/15\n",
      "50/50 [==============================] - 19s 388ms/step - loss: 2.7964e-04 - accuracy: 1.0000 - val_loss: 3.3254e-04 - val_accuracy: 1.0000\n",
      "Epoch 8/15\n",
      "50/50 [==============================] - 20s 396ms/step - loss: 1.8905e-04 - accuracy: 1.0000 - val_loss: 2.1447e-04 - val_accuracy: 1.0000\n",
      "Epoch 9/15\n",
      "50/50 [==============================] - 19s 384ms/step - loss: 1.7288e-04 - accuracy: 1.0000 - val_loss: 1.6006e-04 - val_accuracy: 1.0000\n",
      "Epoch 10/15\n",
      "50/50 [==============================] - 20s 406ms/step - loss: 7.8970e-04 - accuracy: 1.0000 - val_loss: 1.2959e-04 - val_accuracy: 1.0000\n",
      "Epoch 11/15\n",
      "50/50 [==============================] - 19s 385ms/step - loss: 0.4394 - accuracy: 0.8706 - val_loss: 0.0824 - val_accuracy: 0.9987\n",
      "Epoch 12/15\n",
      "50/50 [==============================] - 20s 389ms/step - loss: 0.0236 - accuracy: 0.9940 - val_loss: 0.0043 - val_accuracy: 1.0000\n",
      "Epoch 13/15\n",
      "50/50 [==============================] - 20s 393ms/step - loss: 0.0053 - accuracy: 0.9994 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
      "Epoch 14/15\n",
      "50/50 [==============================] - 20s 396ms/step - loss: 0.0031 - accuracy: 0.9991 - val_loss: 1.1131e-04 - val_accuracy: 1.0000\n",
      "Epoch 15/15\n",
      "50/50 [==============================] - 36s 723ms/step - loss: 4.8792e-04 - accuracy: 1.0000 - val_loss: 1.2735e-04 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Importaciones necesarias\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from keras import models\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import layers\n",
    "\n",
    "# Define una variable TRAIN_PATH con la ruta del directorio que contiene los datos de entrenamiento\n",
    "TRAIN_PATH = '../Proyecto Mlearning/DataSet/Training'\n",
    "\n",
    "# Crear un ImageDataGenerator con los parámetros de data augmentation para el entrenamiento\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255.,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2  # Aquí especificas el porcentaje para validación\n",
    ")\n",
    "\n",
    "# Crear un ImageDataGenerator para la validación (sin data augmentation)\n",
    "validation_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255.,\n",
    "    validation_split=0.2  # Especificar el mismo porcentaje que para el entrenamiento\n",
    ")\n",
    "\n",
    "# Crear el generador de datos de entrenamiento\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    TRAIN_PATH,\n",
    "    target_size=(100, 100),\n",
    "    batch_size=64,\n",
    "    class_mode='categorical',\n",
    "    subset='training',  # Especificar el subset de entrenamiento\n",
    "    seed=123\n",
    ")\n",
    "\n",
    "# Crear el generador de datos de validación\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    TRAIN_PATH,\n",
    "    target_size=(100, 100),\n",
    "    batch_size=64,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',  # Especificar el subset de validación\n",
    "    seed=123\n",
    ")\n",
    "#---------------------------------------------\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(100, 100, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(4, activation='softmax'))\n",
    "#---------------------------------------------\n",
    "# Construcción del modelo (igual\n",
    "#model = Sequential([\n",
    "#    Conv2D(32, (3, 3), activation='relu', input_shape=(100, 100, 3)),\n",
    "#    MaxPooling2D((2, 2)),\n",
    "#    # Puedes agregar más capas convolucionales y de pooling si es necesario\n",
    "#    Flatten(),\n",
    "#    Dense(128, activation='relu', kernel_regularizer=l2(0.001)),  # Regularización L2\n",
    "#    Dropout(0.5),  # Capa de Dropout\n",
    "#    Dense(4, activation='softmax')\n",
    "#])\n",
    "\n",
    "# Compilación del modelo\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=15,\n",
    "    steps_per_epoch=train_generator.n // train_generator.batch_size,\n",
    "    validation_steps=validation_generator.n // validation_generator.batch_size\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 21ms/step\n",
      "Clase predicha: pomelos con confianza: 100.00%\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "# Cargar la imagen de prueba\n",
    "img_path = 'DataSet/TetsY/pomelo.jpg'  # Reemplaza con la ruta de tu imagen de prueba\n",
    "img = image.load_img(img_path, target_size=(100, 100))\n",
    "img_array = image.img_to_array(img)\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "img_array /= 255.0  # Normalizar los valores de píxeles al rango [0, 1]\n",
    "\n",
    "# Realizar la predicción\n",
    "predictions = model.predict(img_array)\n",
    "\n",
    "# Obtener la clase predicha\n",
    "predicted_class = np.argmax(predictions)\n",
    "\n",
    "confidence = predictions[0, predicted_class]\n",
    "\n",
    "# Definir un umbral de confianza\n",
    "umbral_confianza = 0.74\n",
    "\n",
    "# Verificar si la confianza supera el umbral\n",
    "if confidence >= umbral_confianza:\n",
    "    # Imprimir el resultado si la confianza es suficiente\n",
    "    classes = ['limones', 'mandarinas', 'pomelos', 'naranjas']\n",
    "    print(f\"Clase predicha: {classes[predicted_class]} con confianza: {confidence:.2%}\")\n",
    "else:\n",
    "    # Imprimir un mensaje si la confianza es insuficiente\n",
    "    print(\"No estoy seguro de qué clase es esta imagen.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='validation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.preprocessing.image.ImageDataGenerator at 0x3058f97f0>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eda_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
